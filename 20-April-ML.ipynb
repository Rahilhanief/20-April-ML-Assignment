{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319ab72a-f1f1-4c52-813a-f2e266806b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier,\\nwhich uses proximity to make classifications or predictions about the grouping of an individual data point.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 1 :\n",
    "\"\"\"\n",
    "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier,\n",
    "which uses proximity to make classifications or predictions about the grouping of an individual data point.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379af3ba-bea1-456f-9369-81da19a8758e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe optimal K value usually found is the square root of N, where N is the total number of samples.\\nUse an error plot or accuracy plot to find the most favorable K value. \\nKNN performs well with multi-label classes, but you must be aware of the outliers\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 2 :\n",
    "\"\"\"\n",
    "The optimal K value usually found is the square root of N, where N is the total number of samples.\n",
    "Use an error plot or accuracy plot to find the most favorable K value. \n",
    "KNN performs well with multi-label classes, but you must be aware of the outliers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8cef9a-4675-4c04-8dda-af1905db88f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe difference between the KNN classifier and KNN regression methods is that the classifier is used in situations\\nwhere the response variable is categorical (qualitative), \\nwhile the regressor is used in numerical situations (quantitative).\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 3 :\n",
    "\"\"\"\n",
    "The difference between the KNN classifier and KNN regression methods is that the classifier is used in situations\n",
    "where the response variable is categorical (qualitative), \n",
    "while the regressor is used in numerical situations (quantitative).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e031e966-748b-45ee-863b-dbf204ab3d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe main concept for k-NN depends on calculating the distances between the tested,\\nand the training data samples in order to identify its nearest neighbours. \\nThe tested sample is then simply assigned to the class of its nearest neighbour.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 4 :\n",
    "\"\"\"\n",
    "The main concept for k-NN depends on calculating the distances between the tested,\n",
    "and the training data samples in order to identify its nearest neighbours. \n",
    "The tested sample is then simply assigned to the class of its nearest neighbour.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a54ed139-2a68-45cf-b87e-60690d787f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton \\nof space in high-dimensional data sets. \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q no. 5 :\n",
    "\"\"\"The “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton \n",
    "of space in high-dimensional data sets. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af41a9b9-af8e-4c7b-879e-ad2d73ec1e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space.\\nThen we use these 'k' samples to estimate the value of the missing data points. \\nEach sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q no. 6 :\n",
    "\"\"\"\n",
    "The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space.\n",
    "Then we use these 'k' samples to estimate the value of the missing data points. \n",
    "Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc731be6-5f02-451e-9d70-f9a7f66d2dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe key differences are: KNN regression tries to predict the value of the output variable by using a local average.\\nKNN classification attempts to predict the class to which the output variable belong by computing the local probability.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 7 :\n",
    "\"\"\"\n",
    "The key differences are: KNN regression tries to predict the value of the output variable by using a local average.\n",
    "KNN classification attempts to predict the class to which the output variable belong by computing the local probability.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f435ad-ee80-4cc5-a734-3596223b2f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAdvantages and disadvantages of KNN\\nIt's easy to understand and simple to implement.\\nIt can be used for both classification and regression problems.\\nIt's ideal for non-linear data since there's no assumption about underlying data.\\nIt can naturally handle multi-class cases.\\nIt can perform well with enough representative data\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 8 :\n",
    "\"\"\"\n",
    "Advantages and disadvantages of KNN\n",
    "It's easy to understand and simple to implement.\n",
    "It can be used for both classification and regression problems.\n",
    "It's ideal for non-linear data since there's no assumption about underlying data.\n",
    "It can naturally handle multi-class cases.\n",
    "It can perform well with enough representative data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2672cb8d-a19f-4bf2-af0d-e15d528e5715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEuclidean distance is the shortest path between source and destination which is a straight line \\nbut Manhattan distance is sum of all the real distances between source(s) and destination(d) \\nand each distance are always the straight lines.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 9 :\n",
    "\"\"\"\n",
    "Euclidean distance is the shortest path between source and destination which is a straight line \n",
    "but Manhattan distance is sum of all the real distances between source(s) and destination(d) \n",
    "and each distance are always the straight lines.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11bb1e94-b9a9-4639-9e59-c391aed8a09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYes, feature scaling is required to get the better performance of the KNN algorithm.\\nFor Example, Imagine a dataset having n number of instances and N number of features.\\nThere is one feature having values ranging between 0 and 1. Meanwhile, \\nthere is also a feature that varies from -999 to 999. \\nWhen these values are substituted in the formula of Euclidean Distance, this will affect the performance\\nby giving higher weightage to variables having a higher magnitude.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 10 :\n",
    "\"\"\"\n",
    "Yes, feature scaling is required to get the better performance of the KNN algorithm.\n",
    "For Example, Imagine a dataset having n number of instances and N number of features.\n",
    "There is one feature having values ranging between 0 and 1. Meanwhile, \n",
    "there is also a feature that varies from -999 to 999. \n",
    "When these values are substituted in the formula of Euclidean Distance, this will affect the performance\n",
    "by giving higher weightage to variables having a higher magnitude.\n",
    "\"\"\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
